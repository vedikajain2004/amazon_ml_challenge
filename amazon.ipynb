{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "QPxst0fv-xT-",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:21:51.092997Z",
     "start_time": "2025-10-31T13:20:37.785618Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import easyocr\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T13:21:51.195322Z",
     "start_time": "2025-10-31T13:21:51.129817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(42)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(\"C:/Users/jain2/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/jain2/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv\")"
   ],
   "metadata": {
    "id": "5Sg8sf79UUae",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:21:53.451847Z",
     "start_time": "2025-10-31T13:21:51.890966Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T13:21:54.273490Z",
     "start_time": "2025-10-31T13:21:53.552578Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Q4RGxCmYLNc",
    "outputId": "e4bfd3f1-8cc5-4177-c72c-6a07623d0d1a",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:21:56.345075Z",
     "start_time": "2025-10-31T13:21:54.286971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "\n",
    "\n",
    "def extract_text_from_image(url):\n",
    "    try:\n",
    "        results = reader.readtext(url, detail=0)\n",
    "        return \" \".join(results)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def process_batch_train(df, batch_id, save_dir=\"ocr_cache\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = os.path.join(save_dir, f\"train_batch_{batch_id}.csv\")\n",
    "    if os.path.exists(out_path):  # skip if already done\n",
    "        print(f\"Skipping batch {batch_id}, already processed.\")\n",
    "        return\n",
    "\n",
    "    with Pool(processes=min(4, cpu_count() // 2)) as pool:  # safe for laptops\n",
    "        texts = list(tqdm(pool.imap(extract_text_from_image, df[\"image_link\"]), total=len(df)))\n",
    "    df[\"ocr_text\"] = texts\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {out_path}\")\n",
    "\n",
    "def process_batch_test(df, batch_id, save_dir=\"ocr_cache\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    out_path = os.path.join(save_dir, f\"test_batch_{batch_id}.csv\")\n",
    "    if os.path.exists(out_path):  # skip if already done\n",
    "        print(f\"Skipping batch {batch_id}, already processed.\")\n",
    "        return\n",
    "\n",
    "    with Pool(processes=min(4, cpu_count() // 2)) as pool:  # safe for laptops\n",
    "        texts = list(tqdm(pool.imap(extract_text_from_image, df[\"image_link\"]), total=len(df)))\n",
    "    df[\"ocr_text\"] = texts\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {out_path}\")\n",
    "\n",
    "\n",
    "# run in chunks to avoid memory overload\n",
    "batch_size = 2000\n",
    "for i in range(0, 75000, batch_size):\n",
    "    batch_id = i // batch_size\n",
    "    batch_df_train = train_df.iloc[i:i + batch_size].copy()\n",
    "    process_batch_train(batch_df_train, batch_id)\n",
    "for i in range(0, 75000, batch_size):\n",
    "    batch_id = i // batch_size\n",
    "    batch_df_test = test_df.iloc[i:i + batch_size].copy()\n",
    "    process_batch_test(batch_df_test, batch_id)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 0, already processed.\n",
      "Skipping batch 1, already processed.\n",
      "Skipping batch 2, already processed.\n",
      "Skipping batch 3, already processed.\n",
      "Skipping batch 4, already processed.\n",
      "Skipping batch 5, already processed.\n",
      "Skipping batch 6, already processed.\n",
      "Skipping batch 7, already processed.\n",
      "Skipping batch 8, already processed.\n",
      "Skipping batch 9, already processed.\n",
      "Skipping batch 10, already processed.\n",
      "Skipping batch 11, already processed.\n",
      "Skipping batch 12, already processed.\n",
      "Skipping batch 13, already processed.\n",
      "Skipping batch 14, already processed.\n",
      "Skipping batch 15, already processed.\n",
      "Skipping batch 16, already processed.\n",
      "Skipping batch 17, already processed.\n",
      "Skipping batch 18, already processed.\n",
      "Skipping batch 19, already processed.\n",
      "Skipping batch 20, already processed.\n",
      "Skipping batch 21, already processed.\n",
      "Skipping batch 22, already processed.\n",
      "Skipping batch 23, already processed.\n",
      "Skipping batch 24, already processed.\n",
      "Skipping batch 25, already processed.\n",
      "Skipping batch 26, already processed.\n",
      "Skipping batch 27, already processed.\n",
      "Skipping batch 28, already processed.\n",
      "Skipping batch 29, already processed.\n",
      "Skipping batch 30, already processed.\n",
      "Skipping batch 31, already processed.\n",
      "Skipping batch 32, already processed.\n",
      "Skipping batch 33, already processed.\n",
      "Skipping batch 34, already processed.\n",
      "Skipping batch 35, already processed.\n",
      "Skipping batch 36, already processed.\n",
      "Skipping batch 37, already processed.\n",
      "Skipping batch 0, already processed.\n",
      "Skipping batch 1, already processed.\n",
      "Skipping batch 2, already processed.\n",
      "Skipping batch 3, already processed.\n",
      "Skipping batch 4, already processed.\n",
      "Skipping batch 5, already processed.\n",
      "Skipping batch 6, already processed.\n",
      "Skipping batch 7, already processed.\n",
      "Skipping batch 8, already processed.\n",
      "Skipping batch 9, already processed.\n",
      "Skipping batch 10, already processed.\n",
      "Skipping batch 11, already processed.\n",
      "Skipping batch 12, already processed.\n",
      "Skipping batch 13, already processed.\n",
      "Skipping batch 14, already processed.\n",
      "Skipping batch 15, already processed.\n",
      "Skipping batch 16, already processed.\n",
      "Skipping batch 17, already processed.\n",
      "Skipping batch 18, already processed.\n",
      "Skipping batch 19, already processed.\n",
      "Skipping batch 20, already processed.\n",
      "Skipping batch 21, already processed.\n",
      "Skipping batch 22, already processed.\n",
      "Skipping batch 23, already processed.\n",
      "Skipping batch 24, already processed.\n",
      "Skipping batch 25, already processed.\n",
      "Skipping batch 26, already processed.\n",
      "Skipping batch 27, already processed.\n",
      "Skipping batch 28, already processed.\n",
      "Skipping batch 29, already processed.\n",
      "Skipping batch 30, already processed.\n",
      "Skipping batch 31, already processed.\n",
      "Skipping batch 32, already processed.\n",
      "Skipping batch 33, already processed.\n",
      "Skipping batch 34, already processed.\n",
      "Skipping batch 35, already processed.\n",
      "Skipping batch 36, already processed.\n",
      "Skipping batch 37, already processed.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "import glob\n",
    "train_parts = [pd.read_csv(f) for f in sorted(glob.glob(\"ocr_cache/train_batch_*.csv\"))]\n",
    "train_df = pd.concat(train_parts, ignore_index=True)\n",
    "test_parts = [pd.read_csv(f) for f in sorted(glob.glob(\"ocr_cache/test_batch_*.csv\"))]\n",
    "test_df = pd.concat(test_parts, ignore_index=True)"
   ],
   "metadata": {
    "id": "6SKK1KV8YRpv",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:00.890066Z",
     "start_time": "2025-10-31T13:21:56.358087Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "train_df[\"combined_text\"] = train_df[\"catalog_content\"].astype(str) + \" \" + train_df[\"ocr_text\"].astype(str)\n",
    "test_df[\"combined_text\"] = test_df[\"catalog_content\"].astype(str) + \" \" + test_df[\"ocr_text\"].astype(str)"
   ],
   "metadata": {
    "id": "5ygkxBCWbRTp",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:01.142524Z",
     "start_time": "2025-10-31T13:22:00.906757Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "class PricingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, is_train=True):\n",
    "        self.texts = df[\"combined_text\"].tolist()\n",
    "        self.is_train = is_train\n",
    "        if is_train:\n",
    "            self.labels = df[\"price\"].values.astype(float)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        if self.is_train:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ],
   "metadata": {
    "id": "oaXubsjcjC9A",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:01.163897Z",
     "start_time": "2025-10-31T13:22:01.159351Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1  # regression\n",
    ")"
   ],
   "metadata": {
    "id": "IB6QCC4YjFE7",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:05.090517Z",
     "start_time": "2025-10-31T13:22:01.182034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = PricingDataset(train_data, tokenizer)\n",
    "val_dataset = PricingDataset(val_data, tokenizer)\n",
    "test_dataset = PricingDataset(test_df, tokenizer, is_train=False)"
   ],
   "metadata": {
    "id": "TCwrUCpDjJGK",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:05.121817Z",
     "start_time": "2025-10-31T13:22:05.097192Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def smape(preds, labels):\n",
    "    return np.mean(100 * np.abs(preds - labels) / ((np.abs(labels) + np.abs(preds)) / 2))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.flatten()\n",
    "    return {\"smape\": smape(preds, labels)}"
   ],
   "metadata": {
    "id": "96_32qa8jOoJ",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:05.309491Z",
     "start_time": "2025-10-31T13:22:05.145783Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "bYXYODqKjLQF",
    "ExecuteTime": {
     "end_time": "2025-10-31T13:22:05.454881Z",
     "start_time": "2025-10-31T13:22:05.345484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"smape\",\n",
    "    greater_is_better=False,\n",
    "    seed=42,\n",
    "    data_seed=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "id": "TRzGPpczjQv2",
    "ExecuteTime": {
     "end_time": "2025-10-31T14:27:03.818189Z",
     "start_time": "2025-10-31T13:22:05.481406Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jain2\\AppData\\Local\\Temp\\ipykernel_8424\\1406855020.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84380' max='84380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84380/84380 1:04:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Smape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>667.521300</td>\n",
       "      <td>948.456116</td>\n",
       "      <td>61.110756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>596.750300</td>\n",
       "      <td>807.391357</td>\n",
       "      <td>56.185272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>553.761300</td>\n",
       "      <td>718.272156</td>\n",
       "      <td>57.877758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>585.126600</td>\n",
       "      <td>727.791443</td>\n",
       "      <td>54.957657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2879.083700</td>\n",
       "      <td>688.606262</td>\n",
       "      <td>54.440464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>437.864200</td>\n",
       "      <td>674.655457</td>\n",
       "      <td>51.630123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>377.532700</td>\n",
       "      <td>675.284546</td>\n",
       "      <td>52.262157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>448.070700</td>\n",
       "      <td>678.585510</td>\n",
       "      <td>59.836632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>408.858000</td>\n",
       "      <td>668.297546</td>\n",
       "      <td>52.646599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>304.372300</td>\n",
       "      <td>675.798950</td>\n",
       "      <td>56.352757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84380, training_loss=688.0769259043908, metrics={'train_runtime': 3898.0391, 'train_samples_per_second': 173.164, 'train_steps_per_second': 21.647, 'total_flos': 4839086707200000.0, 'train_loss': 688.0769259043908, 'epoch': 10.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "preds = trainer.predict(test_dataset).predictions.flatten()\n",
    "preds = np.maximum(preds, 0)  # enforce positive prices"
   ],
   "metadata": {
    "id": "s18YB5BUjTMo",
    "ExecuteTime": {
     "end_time": "2025-10-31T14:29:17.189385Z",
     "start_time": "2025-10-31T14:27:03.902552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": preds\n",
    "})\n",
    "submission.to_csv(\"test_out.csv\", index=False)\n",
    "print(\"Submission saved to test_out.csv\")"
   ],
   "metadata": {
    "id": "zFhFV26VjVY-",
    "ExecuteTime": {
     "end_time": "2025-10-31T14:29:17.306095Z",
     "start_time": "2025-10-31T14:29:17.227886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to test_out.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Pk7Yo5WWjYQn",
    "ExecuteTime": {
     "end_time": "2025-10-31T14:29:17.346014Z",
     "start_time": "2025-10-31T14:29:17.344052Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T14:29:17.380997Z",
     "start_time": "2025-10-31T14:29:17.379343Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
